{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seasonal Flu Vaccine Predictive Model\n",
    "\n",
    "* **Student name:** Caroline Surratt\n",
    "* **Student pace:** Self-Paced\n",
    "* **Scheduled project review date/time:** Tuesday, October 3rd at 10:00 AM\n",
    "* **Instructor name:** Morgan Jones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Data and Exploratory Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, I will import the features and the target variable using Pandas.\n",
    "\n",
    "The features are stored in the file titled \"training_features\", and the target variable is stored in the file titled \"training_labels\". Both files are located in the data folder of this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "X = pd.read_csv('training_features', index_col='respondent_id')\n",
    "y = pd.read_csv('training_labels', index_col='respondent_id')['seasonal_vaccine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26707, 35)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, as noted in the Data Understanding section, this dataset contains 26,707 entries, with each entry containing information about 35 features. These features will be discussed in more detail below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before any any exploratory analysis or model creation, I will split the data into a training set and a test set. This must occur before any data cleaning or fitting of the model in order to ensure that the model will be appropriate on future unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h1n1_concern</th>\n",
       "      <th>h1n1_knowledge</th>\n",
       "      <th>behavioral_antiviral_meds</th>\n",
       "      <th>behavioral_avoidance</th>\n",
       "      <th>behavioral_face_mask</th>\n",
       "      <th>behavioral_wash_hands</th>\n",
       "      <th>behavioral_large_gatherings</th>\n",
       "      <th>behavioral_outside_home</th>\n",
       "      <th>behavioral_touch_face</th>\n",
       "      <th>doctor_recc_h1n1</th>\n",
       "      <th>...</th>\n",
       "      <th>income_poverty</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>rent_or_own</th>\n",
       "      <th>employment_status</th>\n",
       "      <th>hhs_geo_region</th>\n",
       "      <th>census_msa</th>\n",
       "      <th>household_adults</th>\n",
       "      <th>household_children</th>\n",
       "      <th>employment_industry</th>\n",
       "      <th>employment_occupation</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>respondent_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25194</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Not Married</td>\n",
       "      <td>Own</td>\n",
       "      <td>Not in Labor Force</td>\n",
       "      <td>oxchjgsf</td>\n",
       "      <td>Non-MSA</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14006</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Married</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Employed</td>\n",
       "      <td>lzgpxyit</td>\n",
       "      <td>MSA, Not Principle  City</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>fcxhlnwr</td>\n",
       "      <td>oijqvulv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11285</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;= $75,000, Above Poverty</td>\n",
       "      <td>Not Married</td>\n",
       "      <td>Own</td>\n",
       "      <td>Employed</td>\n",
       "      <td>kbazzjca</td>\n",
       "      <td>MSA, Principle City</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>wlfvacwt</td>\n",
       "      <td>hfxkjkmi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2900</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Below Poverty</td>\n",
       "      <td>Not Married</td>\n",
       "      <td>Own</td>\n",
       "      <td>Employed</td>\n",
       "      <td>mlyzmhmf</td>\n",
       "      <td>MSA, Not Principle  City</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>mcubkhph</td>\n",
       "      <td>ukymxvdu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19083</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bhuqouqj</td>\n",
       "      <td>MSA, Not Principle  City</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21575</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>&gt; $75,000</td>\n",
       "      <td>Not Married</td>\n",
       "      <td>Own</td>\n",
       "      <td>Not in Labor Force</td>\n",
       "      <td>qufhixun</td>\n",
       "      <td>MSA, Principle City</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5390</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;= $75,000, Above Poverty</td>\n",
       "      <td>Not Married</td>\n",
       "      <td>Own</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>mlyzmhmf</td>\n",
       "      <td>MSA, Principle City</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;= $75,000, Above Poverty</td>\n",
       "      <td>Married</td>\n",
       "      <td>Own</td>\n",
       "      <td>Employed</td>\n",
       "      <td>qufhixun</td>\n",
       "      <td>Non-MSA</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>atmlpfrs</td>\n",
       "      <td>xqwwgdyp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15795</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>&gt; $75,000</td>\n",
       "      <td>Married</td>\n",
       "      <td>Own</td>\n",
       "      <td>Employed</td>\n",
       "      <td>kbazzjca</td>\n",
       "      <td>MSA, Principle City</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>fcxhlnwr</td>\n",
       "      <td>cmhcxjea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23654</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;= $75,000, Above Poverty</td>\n",
       "      <td>Not Married</td>\n",
       "      <td>Rent</td>\n",
       "      <td>Employed</td>\n",
       "      <td>fpwskwrf</td>\n",
       "      <td>MSA, Not Principle  City</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>pxcmvdjn</td>\n",
       "      <td>uqqtjvyb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20030 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               h1n1_concern  h1n1_knowledge  behavioral_antiviral_meds  \\\n",
       "respondent_id                                                            \n",
       "25194                   1.0             1.0                        0.0   \n",
       "14006                   2.0             1.0                        0.0   \n",
       "11285                   0.0             0.0                        0.0   \n",
       "2900                    1.0             1.0                        0.0   \n",
       "19083                   2.0             1.0                        1.0   \n",
       "...                     ...             ...                        ...   \n",
       "21575                   2.0             1.0                        0.0   \n",
       "5390                    1.0             1.0                        0.0   \n",
       "860                     2.0             1.0                        0.0   \n",
       "15795                   2.0             1.0                        0.0   \n",
       "23654                   3.0             1.0                        0.0   \n",
       "\n",
       "               behavioral_avoidance  behavioral_face_mask  \\\n",
       "respondent_id                                               \n",
       "25194                           0.0                   0.0   \n",
       "14006                           1.0                   0.0   \n",
       "11285                           0.0                   0.0   \n",
       "2900                            0.0                   0.0   \n",
       "19083                           1.0                   0.0   \n",
       "...                             ...                   ...   \n",
       "21575                           0.0                   0.0   \n",
       "5390                            0.0                   0.0   \n",
       "860                             1.0                   0.0   \n",
       "15795                           1.0                   0.0   \n",
       "23654                           0.0                   0.0   \n",
       "\n",
       "               behavioral_wash_hands  behavioral_large_gatherings  \\\n",
       "respondent_id                                                       \n",
       "25194                            0.0                          0.0   \n",
       "14006                            1.0                          0.0   \n",
       "11285                            0.0                          0.0   \n",
       "2900                             0.0                          0.0   \n",
       "19083                            1.0                          1.0   \n",
       "...                              ...                          ...   \n",
       "21575                            1.0                          0.0   \n",
       "5390                             1.0                          0.0   \n",
       "860                              0.0                          0.0   \n",
       "15795                            1.0                          0.0   \n",
       "23654                            1.0                          0.0   \n",
       "\n",
       "               behavioral_outside_home  behavioral_touch_face  \\\n",
       "respondent_id                                                   \n",
       "25194                              0.0                    0.0   \n",
       "14006                              0.0                    0.0   \n",
       "11285                              0.0                    0.0   \n",
       "2900                               0.0                    0.0   \n",
       "19083                              1.0                    1.0   \n",
       "...                                ...                    ...   \n",
       "21575                              0.0                    1.0   \n",
       "5390                               0.0                    1.0   \n",
       "860                                0.0                    1.0   \n",
       "15795                              0.0                    1.0   \n",
       "23654                              1.0                    1.0   \n",
       "\n",
       "               doctor_recc_h1n1  ...             income_poverty  \\\n",
       "respondent_id                    ...                              \n",
       "25194                       NaN  ...                        NaN   \n",
       "14006                       0.0  ...                        NaN   \n",
       "11285                       0.0  ...  <= $75,000, Above Poverty   \n",
       "2900                        0.0  ...              Below Poverty   \n",
       "19083                       1.0  ...                        NaN   \n",
       "...                         ...  ...                        ...   \n",
       "21575                       0.0  ...                  > $75,000   \n",
       "5390                        NaN  ...  <= $75,000, Above Poverty   \n",
       "860                         0.0  ...  <= $75,000, Above Poverty   \n",
       "15795                       1.0  ...                  > $75,000   \n",
       "23654                       0.0  ...  <= $75,000, Above Poverty   \n",
       "\n",
       "               marital_status  rent_or_own   employment_status  \\\n",
       "respondent_id                                                    \n",
       "25194             Not Married          Own  Not in Labor Force   \n",
       "14006                 Married          NaN            Employed   \n",
       "11285             Not Married          Own            Employed   \n",
       "2900              Not Married          Own            Employed   \n",
       "19083                     NaN          NaN                 NaN   \n",
       "...                       ...          ...                 ...   \n",
       "21575             Not Married          Own  Not in Labor Force   \n",
       "5390              Not Married          Own          Unemployed   \n",
       "860                   Married          Own            Employed   \n",
       "15795                 Married          Own            Employed   \n",
       "23654             Not Married         Rent            Employed   \n",
       "\n",
       "               hhs_geo_region                census_msa  household_adults  \\\n",
       "respondent_id                                                               \n",
       "25194                oxchjgsf                   Non-MSA               1.0   \n",
       "14006                lzgpxyit  MSA, Not Principle  City               2.0   \n",
       "11285                kbazzjca       MSA, Principle City               0.0   \n",
       "2900                 mlyzmhmf  MSA, Not Principle  City               0.0   \n",
       "19083                bhuqouqj  MSA, Not Principle  City               NaN   \n",
       "...                       ...                       ...               ...   \n",
       "21575                qufhixun       MSA, Principle City               0.0   \n",
       "5390                 mlyzmhmf       MSA, Principle City               0.0   \n",
       "860                  qufhixun                   Non-MSA               1.0   \n",
       "15795                kbazzjca       MSA, Principle City               1.0   \n",
       "23654                fpwskwrf  MSA, Not Principle  City               0.0   \n",
       "\n",
       "               household_children  employment_industry  employment_occupation  \n",
       "respondent_id                                                                  \n",
       "25194                         1.0                  NaN                    NaN  \n",
       "14006                         1.0             fcxhlnwr               oijqvulv  \n",
       "11285                         1.0             wlfvacwt               hfxkjkmi  \n",
       "2900                          0.0             mcubkhph               ukymxvdu  \n",
       "19083                         NaN                  NaN                    NaN  \n",
       "...                           ...                  ...                    ...  \n",
       "21575                         0.0                  NaN                    NaN  \n",
       "5390                          0.0                  NaN                    NaN  \n",
       "860                           0.0             atmlpfrs               xqwwgdyp  \n",
       "15795                         0.0             fcxhlnwr               cmhcxjea  \n",
       "23654                         0.0             pxcmvdjn               uqqtjvyb  \n",
       "\n",
       "[20030 rows x 35 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 20030 entries, 25194 to 23654\n",
      "Data columns (total 35 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   h1n1_concern                 19963 non-null  float64\n",
      " 1   h1n1_knowledge               19943 non-null  float64\n",
      " 2   behavioral_antiviral_meds    19974 non-null  float64\n",
      " 3   behavioral_avoidance         19873 non-null  float64\n",
      " 4   behavioral_face_mask         20016 non-null  float64\n",
      " 5   behavioral_wash_hands        19994 non-null  float64\n",
      " 6   behavioral_large_gatherings  19960 non-null  float64\n",
      " 7   behavioral_outside_home      19972 non-null  float64\n",
      " 8   behavioral_touch_face        19932 non-null  float64\n",
      " 9   doctor_recc_h1n1             18395 non-null  float64\n",
      " 10  doctor_recc_seasonal         18395 non-null  float64\n",
      " 11  chronic_med_condition        19313 non-null  float64\n",
      " 12  child_under_6_months         19425 non-null  float64\n",
      " 13  health_worker                19433 non-null  float64\n",
      " 14  health_insurance             10797 non-null  float64\n",
      " 15  opinion_h1n1_vacc_effective  19731 non-null  float64\n",
      " 16  opinion_h1n1_risk            19738 non-null  float64\n",
      " 17  opinion_h1n1_sick_from_vacc  19729 non-null  float64\n",
      " 18  opinion_seas_vacc_effective  19681 non-null  float64\n",
      " 19  opinion_seas_risk            19643 non-null  float64\n",
      " 20  opinion_seas_sick_from_vacc  19623 non-null  float64\n",
      " 21  age_group                    20030 non-null  object \n",
      " 22  education                    18990 non-null  object \n",
      " 23  race                         20030 non-null  object \n",
      " 24  sex                          20030 non-null  object \n",
      " 25  income_poverty               16761 non-null  object \n",
      " 26  marital_status               18992 non-null  object \n",
      " 27  rent_or_own                  18518 non-null  object \n",
      " 28  employment_status            18949 non-null  object \n",
      " 29  hhs_geo_region               20030 non-null  object \n",
      " 30  census_msa                   20030 non-null  object \n",
      " 31  household_adults             19842 non-null  float64\n",
      " 32  household_children           19842 non-null  float64\n",
      " 33  employment_industry          10056 non-null  object \n",
      " 34  employment_occupation        9956 non-null   object \n",
      "dtypes: float64(23), object(12)\n",
      "memory usage: 5.5+ MB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "employment_occupation          13470\n",
       "employment_industry            13330\n",
       "health_insurance               12274\n",
       "income_poverty                  4423\n",
       "doctor_recc_h1n1                2160\n",
       "doctor_recc_seasonal            2160\n",
       "rent_or_own                     2042\n",
       "employment_status               1463\n",
       "marital_status                  1408\n",
       "education                       1407\n",
       "chronic_med_condition            971\n",
       "child_under_6_months             820\n",
       "health_worker                    804\n",
       "opinion_seas_sick_from_vacc      537\n",
       "opinion_seas_risk                514\n",
       "opinion_seas_vacc_effective      462\n",
       "opinion_h1n1_sick_from_vacc      395\n",
       "opinion_h1n1_vacc_effective      391\n",
       "opinion_h1n1_risk                388\n",
       "household_children               249\n",
       "household_adults                 249\n",
       "behavioral_avoidance             208\n",
       "behavioral_touch_face            128\n",
       "h1n1_knowledge                   116\n",
       "h1n1_concern                      92\n",
       "behavioral_large_gatherings       87\n",
       "behavioral_outside_home           82\n",
       "behavioral_antiviral_meds         71\n",
       "behavioral_wash_hands             42\n",
       "behavioral_face_mask              19\n",
       "sex                                0\n",
       "race                               0\n",
       "hhs_geo_region                     0\n",
       "census_msa                         0\n",
       "age_group                          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.isna().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of numeric columns: 23\n",
      "Number of categorical columns: 12\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of numeric columns: {}\".format(len(X_train.select_dtypes(exclude=\"object\").columns)))\n",
    "print(\"Number of categorical columns: {}\".format(len(X_train.select_dtypes(include=\"object\").columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 23 of these features are numerical and 12 are categorical. As can be seen in the above DataFrame, some of the categorical values are descriptive (i.e. the values in the \"rent_or_own\" column are \"Rent\" and \"Own\") while other categorical values are random strings (i.e. the values in the \"employment_industry\" column.\n",
    " \n",
    "There are several features that have a significant number of missing values. These values will need to be imputed in order to build a model that can make predictions on data that may also include missing values. The specific strategy for imputing will be discussed in the preprocessing section below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Create the subplots\u001b[39;00m\n\u001b[1;32m      4\u001b[0m fig, axes \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(nrows\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m, ncols\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Create the subplots\n",
    "fig, axes = plt.subplots(nrows=7, ncols=5, figsize=(12, 10))\n",
    "for i, column in enumerate(X_train):\n",
    "    sns.histplot(X_train, ax=axes[i // 7, i % 5]).set_title(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "seasonal_vaccine\n",
       "0    0.531103\n",
       "1    0.468897\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target variable is a binary value that indicates whether an individual did (1) or did not (0) receive their seasonal flu vaccine. In this dataset, approximately 53% of individuals _did_ get vaccinated, and the remaining 47% _did not_ get vaccinated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will fill missing numerical values with the mean value and missing categorical values with the most frequently occurring value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I will split the features into numerical and categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selects only numerical columns\n",
    "X_train_numerical = X_train.select_dtypes(exclude=object)\n",
    "\n",
    "# selects only categorical columns\n",
    "X_train_categorical = X_train.select_dtypes(include=object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I will use SimpleImputer to fill the missing numerical values with the mean of the column and missing categorical values with the most frequently occurring value in the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_numerical.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# instantiates SimpleImputer that will fill missing values with the column mean\n",
    "numerical_imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# fits/transforms the SimpleImputer object with the numerical training data and formats as DataFrame\n",
    "X_train_numerical = pd.DataFrame(numerical_imputer.fit_transform(X_train_numerical),\n",
    "                                columns = X_train_numerical.columns,\n",
    "                                index = X_train_numerical.index)\n",
    "\n",
    "# instantiates SimpleImputer that will fill missing values with most frequent column value\n",
    "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# fits/transforms the SimpleImputer object with the categorical training data and formats as a DataFrame\n",
    "X_train_categorical = pd.DataFrame(categorical_imputer.fit_transform(X_train_categorical),\n",
    "                                  columns = X_train_categorical.columns,\n",
    "                                  index = X_train_categorical.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I will one-hot encode the categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# instantiates OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# fits and transforms OneHotEncoder object on the categorical training data\n",
    "X_train_categorical_ohe = ohe.fit_transform(X_train_categorical)\n",
    "\n",
    "# re-formats the array as a DataFrame (in order to concatenate with numerical training data)\n",
    "X_train_categorical_ohe = pd.DataFrame(X_train_categorical_ohe, \n",
    "                                       columns=ohe.get_feature_names_out(X_train_categorical.columns),\n",
    "                                       index=X_train_categorical.index)\n",
    "\n",
    "X_train_categorical_ohe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing Numeric Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, I will normalize the data in order to prevent variables with larger scales from having a disproportional impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train_numerical = pd.DataFrame(scaler.fit_transform(X_train_numerical),\n",
    "                                index=X_train_numerical.index,\n",
    "                                columns=X_train_numerical.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenating Numerical and Categorical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I will concatenate the numerical and categorical training data into a single DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat([X_train_numerical, X_train_categorical_ohe], axis=1)\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USING LABEL ENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "\n",
    "# selects only numerical columns\n",
    "X_train_numerical = X_train.select_dtypes(exclude=object)\n",
    "\n",
    "# selects only categorical columns\n",
    "X_train_categorical = X_train.select_dtypes(include=object)\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# instantiates SimpleImputer that will fill missing values with the column mean\n",
    "numerical_imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# fits/transforms the SimpleImputer object with the numerical training data and formats as DataFrame\n",
    "X_train_numerical = pd.DataFrame(numerical_imputer.fit_transform(X_train_numerical),\n",
    "                                columns = X_train_numerical.columns,\n",
    "                                index = X_train_numerical.index)\n",
    "\n",
    "# instantiates SimpleImputer that will fill missing values with most frequent column value\n",
    "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# fits/transforms the SimpleImputer object with the categorical training data and formats as a DataFrame\n",
    "X_train_categorical = pd.DataFrame(categorical_imputer.fit_transform(X_train_categorical),\n",
    "                                  columns = X_train_categorical.columns,\n",
    "                                  index = X_train_categorical.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting the Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I will fit a decision tree classifier object on the training data. For the baseline model, I will not adjust any of the hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "baseline_tree = DecisionTreeClassifier(criterion='entropy', random_state=42)\n",
    "\n",
    "baseline_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance on Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I will use the baseline model to predict the target variable for both the training data and the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_train = baseline_tree.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_residuals = np.abs(y_train - y_hat_train)\n",
    "\n",
    "print(pd.Series(train_residuals, name=\"Residuals (counts)\").value_counts())\n",
    "print()\n",
    "print(pd.Series(train_residuals, name=\"Residuals (proportions)\").value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_hat_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance on Testing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selects only numerical columns\n",
    "X_test_numerical = X_test.select_dtypes(exclude=object)\n",
    "\n",
    "# selects only categorical columns\n",
    "X_test_categorical = X_test.select_dtypes(include=object)\n",
    "\n",
    "# transforms the numerical testing data and formats as DataFrame\n",
    "X_test_numerical = pd.DataFrame(numerical_imputer.transform(X_test_numerical),\n",
    "                                columns = X_test_numerical.columns,\n",
    "                                index = X_test_numerical.index)\n",
    "\n",
    "\n",
    "# transforms the categorical testing data and formats as DataFrame\n",
    "X_test_categorical = pd.DataFrame(categorical_imputer.transform(X_test_categorical),\n",
    "                                  columns = X_test_categorical.columns,\n",
    "                                  index = X_test_categorical.index)\n",
    "\n",
    "\n",
    "# One-hot encodes categorical testing data \n",
    "X_test_categorical_ohe = ohe.transform(X_test_categorical)\n",
    "\n",
    "# re-formatts the array as a DataFrame (in order to concatenate with numerical testing data)\n",
    "X_test_categorical_ohe = pd.DataFrame(X_test_categorical_ohe, \n",
    "                                       columns=ohe.get_feature_names_out(X_test_categorical.columns),\n",
    "                                       index=X_test_categorical.index)\n",
    "\n",
    "X_test_numerical = pd.DataFrame(scaler.transform(X_test_numerical),\n",
    "                                index=X_test_numerical.index,\n",
    "                                columns=X_test_numerical.columns)\n",
    "\n",
    "X_test = pd.concat([X_test_numerical, X_test_categorical_ohe], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Testing Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test = baseline_tree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_residuals = np.abs(y_test - y_hat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(pd.Series(test_residuals, name=\"Residuals (counts)\").value_counts())\n",
    "print()\n",
    "print(pd.Series(test_residuals, name=\"Residuals (proportions)\").value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_hat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_hat_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Still need to adjust hyperparameters (plan to use GridSearchCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [1, 2, 5, 7, 10, 15, 20],\n",
    "    'min_samples_split': [5, 10, 20, 30, 50],\n",
    "    'min_samples_leaf': [5, 10, 20, 30, 50]\n",
    "}\n",
    "\n",
    "gs_tree = GridSearchCV(clf, param_grid, cv=3)\n",
    "gs_tree.fit(X_train, y_train)\n",
    "\n",
    "gs_tree.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_tree = DecisionTreeClassifier(criterion='entropy', max_depth=10, min_samples_leaf=50, min_samples_split=5)\n",
    "tuned_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test = tuned_tree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_hat_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(fit_intercept=False, solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_hat_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using LabelEncoder instead of OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old code - ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gini_tree = DecisionTreeClassifier(random_state=42)\n",
    "gini_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline Logistic Regression model in StatsModels\n",
    "import statsmodels.api as sm\n",
    "\n",
    "X_train_for_statsmodels = sm.add_constant(X_train)\n",
    "\n",
    "model = sm.Logit(y_train, X_train_for_statsmodels)\n",
    "result = model.fit()\n",
    "\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(fit_intercept=False, \n",
    "                            C=1e12, \n",
    "                            solver='liblinear')\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_hat_logreg = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_test_hat_logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_test_hat_logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg2 = LogisticRegression(fit_intercept=False,\n",
    "                            C=1e12,\n",
    "                            solver='liblinear')\n",
    "\n",
    "logreg2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_hat_logreg2 = logreg2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_test_hat_logreg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping irrelevant columns from decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('training_features', index_col='respondent_id')\n",
    "y = pd.read_csv('training_labels', index_col='respondent_id')['seasonal_vaccine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['h1n1_concern', 'h1n1_knowledge', 'doctor_recc_h1n1', \n",
    "                   'opinion_h1n1_vacc_effective', 'opinion_h1n1_risk', \n",
    "                   'opinion_h1n1_sick_from_vacc']\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop(columns_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New model after dropped columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# selects only numerical columns\n",
    "X_train_numerical = X_train.select_dtypes(exclude=object)\n",
    "\n",
    "# selects only categorical columns\n",
    "X_train_categorical = X_train.select_dtypes(include=object)\n",
    "\n",
    "# instantiates SimpleImputer that will fill missing values with the column mean\n",
    "numerical_imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# fits the SimpleImputer object on the numerical training data and formats as DataFrame\n",
    "X_train_numerical = pd.DataFrame(numerical_imputer.fit_transform(X_train_numerical),\n",
    "                                columns = X_train_numerical.columns,\n",
    "                                index = X_train_numerical.index)\n",
    "\n",
    "\n",
    "# categorical\n",
    "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "X_train_categorical = pd.DataFrame(categorical_imputer.fit_transform(X_train_categorical),\n",
    "                                  columns = X_train_categorical.columns,\n",
    "                                  index = X_train_categorical.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# instantiated OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# fit and transform ohe on the categorical training data\n",
    "X_train_categorical_ohe = ohe.fit_transform(X_train_categorical)\n",
    "\n",
    "# re-formatted the array as a DataFrame (need column titles and index to concatenate)\n",
    "X_train_categorical_ohe = pd.DataFrame(X_train_categorical_ohe, \n",
    "                                       columns=ohe.get_feature_names_out(X_train_categorical.columns),\n",
    "                                       index=X_train_categorical.index)\n",
    "\n",
    "X_train_categorical_ohe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat([X_train_numerical, X_train_categorical_ohe], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_tree = DecisionTreeClassifier(criterion='entropy', random_state=42)\n",
    "entropy_tree.fit(X_train, y_train)\n",
    "\n",
    "gini_tree = DecisionTreeClassifier(random_state=42)\n",
    "gini_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selects only numerical columns\n",
    "X_test_numerical = X_test.select_dtypes(exclude=object)\n",
    "\n",
    "# selects only categorical columns\n",
    "X_test_categorical = X_test.select_dtypes(include=object)\n",
    "\n",
    "# fills missing values in X_test_numerical with the column mean of training data\n",
    "X_test_numerical = pd.DataFrame(numerical_imputer.transform(X_test_numerical),\n",
    "                               columns = X_test_numerical.columns,\n",
    "                               index = X_test_numerical.index)\n",
    "\n",
    "# fills missing values in X_test_categorical with the column mode of training data\n",
    "X_test_categorical = pd.DataFrame(categorical_imputer.transform(X_test_categorical),\n",
    "                                 columns = X_test_categorical.columns,\n",
    "                                 index = X_test_categorical.index)\n",
    "\n",
    "# one-hot encodes testing data using the ohe object fit on the training data\n",
    "X_test_categorical_ohe = ohe.transform(X_test_categorical)\n",
    "\n",
    "# reformats the array as a DataFrame\n",
    "X_test_categorical_ohe = pd.DataFrame(X_test_categorical_ohe,\n",
    "                                      columns = ohe.get_feature_names_out(X_test_categorical.columns),\n",
    "                                      index = X_test_categorical.index)\n",
    "\n",
    "X_test = pd.concat([X_test_numerical, X_test_categorical_ohe], axis = 1)\n",
    "\n",
    "y_test_hat_entropy = entropy_tree.predict(X_test)\n",
    "print(\"Entropy Test Accuracy: \", accuracy_score(y_test, y_test_hat_entropy))\n",
    "\n",
    "y_test_hat_gini = gini_tree.predict(X_test)\n",
    "print(\"Gini Test Accuracy: \", accuracy_score(y_test, y_test_hat_gini))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to run in terminal: conda install -c conda-forge statsmodels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sklearn-env)",
   "language": "python",
   "name": "sklearn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
